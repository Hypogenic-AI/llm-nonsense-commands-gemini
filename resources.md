## Resources Catalog

### Summary
This document catalogs all resources gathered for the research project, including papers, datasets, and code repositories.

### Papers
Total papers downloaded: 6

| Title | Authors | Year | File | Key Info |
|-------|---------|------|------|----------|
| AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models | Sicheng Zhu, et al. | 2023 | papers/2310.15140v2_AutoDAN.pdf | Interpretable, readable jailbreak prompts. |
| LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts | Darpan Aswal, CÃ©line Hudelot | 2025 | papers/2508.16325v1_LLMSymGuard.pdf | Interpretable jailbreak concepts. |
| The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning | Siyi Chen, et al. | 2025 | papers/2504.21307v2_DualPower.pdf | Interpretable token embeddings for jailbreaking. |
| Adversarial Attacks and Defenses: An Interpretation Perspective | Ninghao Liu, et al. | 2020 | papers/2004.11488v2_AdversarialInterpretation.pdf | Overview of adversarial attacks and interpretability. |
| Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs | Bowen Fan, et al. | 2025 | papers/2510.12233v1_GraphLLMs.pdf | Interpretable attacks on Graph-LLMs. |
| Explain2Attack: Text Adversarial Attacks via Cross-Domain Interpretability | Mahmoud Hossam, et al. | 2020 | papers/2010.06812v4_Explain2Attack.pdf | Interpretable substitute model for attacks. |

See papers/README.md for detailed descriptions.

### Datasets
Total datasets downloaded: 1

| Name | Source | Size | Task | Location | Notes |
|------|--------|------|------|----------|-------|
| JailbreakBench/JBB-Behaviors | HuggingFace | 200 | Adversarial Prompts | datasets/JailbreakBench_JBB-Behaviors/ | Contains harmful and benign prompts. |

See datasets/README.md for detailed descriptions.

### Code Repositories
Total repositories cloned: 2

| Name | URL | Purpose | Location | Notes |
|------|-----|---------|----------|-------|
| AutoDAN-Turbo | github.com/SaFoLab-WISC/AutoDAN-Turbo | Adversarial Attack Generation | code/AutoDAN-Turbo/ | For generating interpretable jailbreak prompts. |
| TextAttack | github.com/QData/TextAttack | Adversarial Attack Library | code/TextAttack/ | General-purpose library for text adversarial attacks. |

See code/README.md for detailed descriptions.

### Resource Gathering Notes

#### Search Strategy
I used a combination of Google Scholar, the arXiv API, and targeted GitHub searches to find relevant resources. The initial broad searches were narrowed down based on the relevance of the titles and abstracts to the research question.

#### Selection Criteria
The main selection criteria were relevance to the research hypothesis, recency of the work, and availability of code or datasets. I prioritized papers that dealt with "interpretable" adversarial attacks and "jailbreaking."

#### Challenges Encountered
The main challenge was the difficulty in using Python libraries within the environment due to dependency issues. This was overcome by using `curl` and `web_fetch` to directly access APIs and download files. Another challenge was that some datasets were gated, requiring a different approach.

#### Gaps and Workarounds
The main gap is the lack of a standardized, large-scale dataset of "nonsense commands." I have addressed this by finding a relevant dataset of jailbreak prompts and a code repository that can be used to generate more prompts.

### Recommendations for Experiment Design

Based on the gathered resources, I recommend:

1. **Primary dataset(s)**: Use the `JailbreakBench/JBB-Behaviors` dataset as a starting point. Augment this dataset with more "nonsense commands" generated using the `AutoDAN-Turbo` code.
2. **Baseline methods**: Use the `TextAttack` library to implement baseline adversarial attacks like PGD and compare them to the interpretable attacks generated by AutoDAN.
3. **Evaluation metrics**: Use a combination of Attack Success Rate and a human-in-the-loop evaluation to measure the "sensicality" of the generated prompts.
4. **Code to adapt/reuse**: The `AutoDAN-Turbo` code should be adapted to generate a diverse set of "nonsense commands" for the experiment.
