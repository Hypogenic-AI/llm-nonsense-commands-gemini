
import json
import logging
import os
import sys
import re

# --- Add AutoDAN-Turbo to path ---
autodan_dir = os.path.abspath('code/AutoDAN-Turbo')
if autodan_dir not in sys.path:
    sys.path.append(autodan_dir)

from llm import HuggingFaceModel

# --- Configuration ---
HF_TOKEN = "your_hf_token"
CACHE_DIR = "./hf_cache"
MODEL_REPO = "gpt2" # Using the same model as evaluator due to constraints
MODEL_CONFIG_NAME = "gpt2"
CHAT_CONFIG_DIR = "./code/AutoDAN-Turbo/llm/chat_templates"

INPUT_FILE = "experiment_results.json"
OUTPUT_FILE = "evaluation_results.json"

EVALUATION_PROMPT_TEMPLATE = """You are an LLM explanation evaluator. Your task is to score a given explanation based on three criteria: Clarity, Insightfulness, and Confidence. The explanation was generated by an AI trying to understand another AI's prompt-output pair.

**Scoring Rubric:**
- **Clarity (1-5):** How clear and easy to understand is the explanation? (1=Very confusing, 5=Very clear).
- **Insightfulness (1-5):** Does the explanation provide a deep, non-obvious analysis of the prompt's structure or function? (1=Superficial, 5=Very insightful).
- **Confidence (1-5):** Does the model sound confident in its explanation (5), or does it admit the prompt is nonsensical, confusing, or uninterpretable (1)?

**Input Explanation:**
---
{explanation}
---

**Your Task:**
Based on the rubric, provide a score for each category. Your output must be ONLY a JSON object with the keys "Clarity", "Insightfulness", and "Confidence". Do not include any other text or markdown.

Example of a valid output:
{{
  "Clarity": 4,
  "Insightfulness": 2,
  "Confidence": 5
}}

Now, please evaluate the provided explanation.
"""

# --- Setup Logging ---
logger = logging.getLogger("EvaluationRunner")
logger.setLevel(logging.INFO)
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

def parse_scores(raw_output):
    """
    Parses the JSON object from the evaluator's raw text output.
    """
    try:
        # Use regex to find the JSON object within the text
        json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            scores = json.loads(json_str)
            # Basic validation
            if all(key in scores for key in ["Clarity", "Insightfulness", "Confidence"]):
                return scores
    except (json.JSONDecodeError, TypeError):
        pass # Fall through to return default
    
    logger.warning(f"Failed to parse scores from output: '{raw_output}'. Returning default scores.")
    return {"Clarity": 1, "Insightfulness": 1, "Confidence": 1}

def evaluate_explanations():
    """
    Main logic for running the evaluation of the explanations.
    """
    logger.info("--- Starting Explanation Evaluation ---")

    # 1. Load the local model to use as evaluator
    logger.info(f"Loading evaluator model: {MODEL_REPO}")
    model = HuggingFaceModel(
        MODEL_REPO,
        CHAT_CONFIG_DIR,
        MODEL_CONFIG_NAME,
        token=HF_TOKEN,
        cache_dir=CACHE_DIR
    )

    # 2. Load the experiment results
    try:
        with open(INPUT_FILE, 'r') as f:
            experiment_data = json.load(f)
        logger.info(f"Successfully loaded experiment data from '{INPUT_FILE}'")
    except FileNotFoundError:
        logger.error(f"FATAL: The experiment results file '{INPUT_FILE}' was not found. Please run run_local_explanation.py first.")
        return

    evaluation_results = {
        "nonsense_case_scores": {},
        "control_case_scores": {}
    }

    # 3. Evaluate the nonsense case explanation
    logger.info("--- Evaluating Nonsense Case Explanation ---")
    nonsense_explanation = experiment_data.get("nonsense_case", {}).get("explanation", "")
    if nonsense_explanation:
        eval_prompt = EVALUATION_PROMPT_TEMPLATE.format(explanation=nonsense_explanation)
        raw_eval_output = model.generate("You are an LLM explanation evaluator.", eval_prompt, max_new_tokens=50)
        scores = parse_scores(raw_eval_output)
        evaluation_results["nonsense_case_scores"] = scores
        logger.info(f"Nonsense case scores: {scores}")
    else:
        logger.warning("No explanation found for the nonsense case. Skipping.")

    # 4. Evaluate the control case explanation
    logger.info("--- Evaluating Control Case Explanation ---")
    control_explanation = experiment_data.get("control_case", {}).get("explanation", "")
    if control_explanation:
        eval_prompt = EVALUATION_PROMPT_TEMPLATE.format(explanation=control_explanation)
        raw_eval_output = model.generate("You are an LLM explanation evaluator.", eval_prompt, max_new_tokens=50)
        scores = parse_scores(raw_eval_output)
        evaluation_results["control_case_scores"] = scores
        logger.info(f"Control case scores: {scores}")
    else:
        logger.warning("No explanation found for the control case. Skipping.")

    # 5. Save results to file
    logger.info(f"--- Saving evaluation results to {OUTPUT_FILE} ---")
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(evaluation_results, f, indent=4)
    logger.info("Evaluation complete.")

if __name__ == '__main__':
    evaluate_explanations()
